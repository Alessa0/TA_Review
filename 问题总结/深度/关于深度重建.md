# 关于深度重建

在某些特定应用场景，比如说屏幕空间反射，会要求我们从深度缓冲中重建像素点的世界空间位置。本文介绍在Unity中如何从深度缓冲中重建世界空间位置。

简介：

- 从深度缓冲重建像素的世界坐标，根据实际使用场景——是渲染网格还是用于图像后处理——可分为两种实现方案：[cyanilux.com/tutorials/](http://link.zhihu.com/?target=https%3A//www.cyanilux.com/tutorials/depth/%23reconstruct-world-pos)

- 不使用相似三角形，直接构建NDC space，对其做逆VP变换，得到 homogeneous world space，除以w分量即可得posWS：[/manual/writing-shaders-urp-reconstruct-world-position.html](http://link.zhihu.com/?target=https%3A//docs.unity3d.com/Packages/com.unity.render-pipelines.universal@17.0/manual/writing-shaders-urp-reconstruct-world-position.html)

## 深度缓冲

首先先来看看在Unity中怎么计算深度。

> UnityCG.cginc

```glsl
#define COMPUTE_EYEDEPTH(o) o = -mul( UNITY_MATRIX_MV, v.vertex ).z
#define COMPUTE_DEPTH_01 -(mul( UNITY_MATRIX_MV, v.vertex ).z * _ProjectionParams.w)
```

其中，_ProjectionParams.w 是 $\Large \frac 1 {FarPlane} $[[参考]](https://zhuanlan.zhihu.com/p/92315967#ref_1)。

符号取反的原因是在Unity的观察空间（View space）中z轴翻转了，摄像机的前向量就是z轴的正方向[[参考]](https://zhuanlan.zhihu.com/p/92315967#ref_2)。这是和OpenGL中不一样的一点。

从上式可知，Unity中的观察线性深度（Eye depth）就是顶点在观察空间（View space）中的z分量，而01线性深度（01 depth）就是观察线性深度通过除以摄像机远平面重新映射到[0，1]区间所得到的值。

我们可以从深度缓冲中采样得到深度值，并使用Unity中内置的功能函数将原始数据转换成线性深度。

> UnityCG.cginc

```glsl
// Z buffer to linear 0..1 depth (0 at eye, 1 at far plane)
inline float Linear01Depth( float z )
{
    return 1.0 / (_ZBufferParams.x * z + _ZBufferParams.y);
}
// Z buffer to linear depth
inline float LinearEyeDepth( float z )
{
    return 1.0 / (_ZBufferParams.z * z + _ZBufferParams.w);
}
```

------

## 从NDC空间中重建

第一种方法是通过像素的屏幕坐标位置来计算。

![img](.\imgs\重建1.png)

首先将屏幕空间坐标转换到NDC空间中。

```glsl
float4 ndcPos = (o.screenPos / o.screenPos.w) * 2 - 1;
```

然后将屏幕像素对应在摄像机远平面（Far plane）的点转换到剪裁空间（Clip space）。因为在NDC空间中远平面上的点的z分量为1，所以可以直接乘以摄像机的Far值来将其转换到剪裁空间（实际就是反向透视除法）。

```glsl
float far = _ProjectionParams.z;
float3 clipVec = float3(ndcPos.x, ndcPos.y, 1.0) * far;
```

接着通过逆投影矩阵（Inverse Projection Matrix）将点转换到观察空间（View space）。

```glsl
float3 o.viewVec = mul(unity_CameraInvProjection, clipVec.xyzz).xyz;
```

已知在观察空间中摄像机的位置一定为（0，0，0），所以从摄像机指向远平面上的点的向量就是其在观察空间中的位置。

将向量乘以线性深度值，得到在深度缓冲中储存的值的观察空间位置。

```glsl
float depth = UNITY_SAMPLE_DEPTH(tex2Dproj(_CameraDepthTexture, i.screenPos));
float3 viewPos = i.viewVec * Linear01Depth(depth);
```

最后将观察空间中的位置变换到世界空间中。

```glsl
float3 worldPos = mul(UNITY_MATRIX_I_V, float4(viewPos, 1.0)).xyz;
```

附上在Shader Graph中的实现。这里Unity有bug导致如果使用Transformation Matrix节点的Inverse Projection会报错，所以这里使用了一个Custom Function节点输出一个4x4矩阵unity_CameraInvProjection。理论上效果是一样的。

![img](.\imgs\重建2.png)

Shader Graph（Custom Function输出unity_CameraInvProjection）

------

## 在世界空间中重建

第二种方法是利用在世界空间中从摄像机指向屏幕像素点的向量来计算。

![img](.\imgs\重建3.png)

首先构造在世界空间中从摄像机指向屏幕像素点的向量。

```glsl
o.worldSpaceDir = WorldSpaceViewDir(v.vertex);
```

将向量转换到观察空间，存储其z分量的值。注意向量和位置的空间转换是不同的，当w分量为0的时候Unity会将其视为向量，而当w分量为1的时候Unity将其视为位置。

```glsl
o.viewSpaceZ = mul(UNITY_MATRIX_V, float4(o.worldSpaceDir, 0.0)).z;
```

在深度缓冲中采样。这里使用tex2Dproj而不是tex2D的原因是screenPos是用ComputeScreenPos来计算得到的[[3\]](https://zhuanlan.zhihu.com/p/92315967#ref_3)，用tex2Dproj可以帮我们做透视除法。

```glsl
float eyeDepth = UNITY_SAMPLE_DEPTH(tex2Dproj(_CameraDepthTexture, i.screenPos));
eyeDepth = LinearEyeDepth(eyeDepth);
```

因为像素点的观察线性深度就是其在观察空间中的z分量，所以根据向量的z分量计算其缩放因子，将向量缩放到实际的长度。

```glsl
i.worldSpaceDir *= -eyeDepth / i.viewSpaceZ;
```

最后以摄像机为起点，缩放后的向量为指向向量，得到像素点在世界空间中位置。

```glsl
float3 worldPos = _WorldSpaceCameraPos + i.worldSpaceDir;
```

附上在Shader Graph中的实现。

![img](.\imgs\重建4.png)

Shader Graph

------

## 正交摄像机的情况

如果摄像机不是透视而是正交的，做法上就有些不同。

计算观察空间（View space）中顶点的xy分量。

```glsl
float4 ndcPos = (o.screenPos / o.screenPos.w) * 2 - 1;
o.viewVec = float3(unity_OrthoParams.xy * ndcPos.xy, 0);
```

正交摄像机的深度缓冲是线性的。根据深度值在远近平面之间作线性插值，就得到顶点在观察空间中的z分量。

```glsl
float depth = UNITY_SAMPLE_DEPTH(tex2D(_CameraDepthTexture, i.screenPos));
float z = -lerp(near, far, depth);
```

最后将观察空间中的位置转换到世界空间。

```glsl
float3 worldPos = mul(UNITY_MATRIX_I_V, float4(viewPos, 1)).xyz;
```

这里不知道为什么Unity对正交摄像机的深度缓冲无法采样出正确的深度值，需要重新变换才能得到正确的结果[[4\]](https://zhuanlan.zhihu.com/p/92315967#ref_4)。

```glsl
// Wrong depth
// float depth = UNITY_SAMPLE_DEPTH(tex2D(_CameraDepthTexture, i.screenPos));

// Correct depth
float rawDepth = UNITY_SAMPLE_DEPTH(tex2D(_CameraDepthTexture, i.screenPos));
float ortho = (far - near) * (1 - rawDepth) + near;
float depth = lerp(LinearEyeDepth(rawDepth), ortho, unity_OrthoParams.w) / far;
```

附上在Shader Graph中的实现。其中Custom Function节点为：

```glsl
float ortho = (_ProjectionParams.z - _ProjectionParams.y) * (1 - rawDepth) 
              + _ProjectionParams.y;
depth =  lerp(eyeDepth, ortho, unity_OrthoParams.w) / _ProjectionParams.z;
```

![img](.\imgs\重建5.png)

Shader Graph

------

以上方法可以根据实际渲染的对象是在世界中物体（贴花）还是屏幕大小的四边形（后处理）来灵活使用。最后附上大神写的代码作为参考，这个项目通过后处理将深度缓冲转换成世界空间位置并可视化。

[keijiro/DepthInverseProjectiongithub.com/keijiro/DepthInverseProjection](https://link.zhihu.com/?target=https%3A//github.com/keijiro/DepthInverseProjection)

**代码**

- NDC方法

```glsl
#pragma vertex vert
#pragma fragment frag

#include "UnityCG.cginc"

struct v2f
{
    float4 vertex : SV_POSITION;
    float4 screenPos : TEXCOORD0;
    float3 viewVec : TEXCOORD1;
};

v2f vert(appdata_base v)
{
    v2f o;
    o.vertex = UnityObjectToClipPos(v.vertex);

    // Compute texture coordinate
    o.screenPos = ComputeScreenPos(o.vertex);

    // NDC position
    float4 ndcPos = (o.screenPos / o.screenPos.w) * 2 - 1;

    // Camera parameter
    float far = _ProjectionParams.z;

    // View space vector pointing to the far plane
    float3 clipVec = float3(ndcPos.x, ndcPos.y, 1.0) * far;
    o.viewVec = mul(unity_CameraInvProjection, clipVec.xyzz).xyz;

    return o;
}

sampler2D _CameraDepthTexture;

half4 frag(v2f i) : SV_Target
{
    // Sample the depth texture to get the linear 01 depth
    float depth = UNITY_SAMPLE_DEPTH(tex2Dproj(_CameraDepthTexture, i.screenPos));
    depth = Linear01Depth(depth);

    // View space position
    float3 viewPos = i.viewVec * depth;

    // Pixel world position
    float3 worldPos = mul(UNITY_MATRIX_I_V, float4(viewPos, 1)).xyz;

    return float4(worldPos, 1.0);
}
```

- 世界空间方法

```glsl
#pragma vertex vert
#pragma fragment frag

#include "UnityCG.cginc"

struct v2f
{
    float4 vertex : SV_POSITION;
    float4 screenPos : TEXCOORD0;
    float3 worldSpaceDir : TEXCOORD1;
    float viewSpaceZ : TEXCOORD2;
};

v2f vert(appdata_base v)
{
    v2f o;
    o.vertex = UnityObjectToClipPos(v.vertex);

    // World space vector from camera to the vertex position
    o.worldSpaceDir = WorldSpaceViewDir(v.vertex);

    // Z value of the vector in view space
    o.viewSpaceZ = mul(UNITY_MATRIX_V, float4(o.worldSpaceDir, 0.0)).z;

    // Compute texture coordinate
    o.screenPos = ComputeScreenPos(o.vertex);
    return o;
}

sampler2D _CameraDepthTexture;

half4 frag(v2f i) : SV_Target
{
    // Sample the depth texture to get the linear eye depth
    float eyeDepth = UNITY_SAMPLE_DEPTH(tex2Dproj(_CameraDepthTexture, i.screenPos));
    eyeDepth = LinearEyeDepth(eyeDepth);

    // Rescale the vector
    i.worldSpaceDir *= -eyeDepth / i.viewSpaceZ;

    // Pixel world position
    float3 worldPos = _WorldSpaceCameraPos + i.worldSpaceDir;

    return float4(worldPos, 1.0);
}
```

- 正交摄像机的情况

```glsl
#pragma vertex vert
#pragma fragment frag

#include "UnityCG.cginc"

struct v2f
{
    float4 vertex : SV_POSITION;
    float4 screenPos : TEXCOORD0;
    float3 viewVec : TEXCOORD1;
};

v2f vert(appdata_base v)
{
    v2f o;
    o.vertex = UnityObjectToClipPos(v.vertex);

    // Compute texture coordinate
    o.screenPos = ComputeScreenPos(o.vertex);

    // NDC position
    float4 ndcPos = (o.screenPos / o.screenPos.w) * 2 - 1;

    // View space vector from near plane pointing to far plane
    o.viewVec = float3(unity_OrthoParams.xy * ndcPos.xy, 0);

    return o;
}

sampler2D _CameraDepthTexture;

half4 frag(v2f i) : SV_Target
{
    // Camera parameters
    float near = _ProjectionParams.y;
    float far = _ProjectionParams.z;

    // Sample the depth texture to get the linear depth
    float rawDepth = UNITY_SAMPLE_DEPTH(tex2D(_CameraDepthTexture, i.screenPos));
    float ortho = (far - near) * (1 - rawDepth) + near;
    float depth = lerp(LinearEyeDepth(rawDepth), ortho, unity_OrthoParams.w) / far;

    // Linear interpolate between near plane and far plane by depth value
    float z = -lerp(near, far, depth);

    // View space position
    float3 viewPos = float3(i.viewVec.xy, z);

    // Pixel world position
    float3 worldPos = mul(UNITY_MATRIX_I_V, float4(viewPos, 1)).xyz;

    return float4(worldPos, 1.0);
}
```

## 两种方法的区别

**对比：**

1. **NDC逆推法：**需要的条件最少，函数只用ScreenPos。但会在片元阶段进行多次矩阵运算，比较耗。
2. **相似[三角形法](https://www.zhihu.com/search?q=三角形法&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A3196788581})：**相比于NDC逆推法，要多算一个posVS。但矩阵运算量变小了。
3. **角向量插值法：**合理的利用了片元阶段的插值，最省性能。但需要额外的C#脚本计算四个角落的向量方向。

### 1.NDC逆推法

先获取屏幕空间位置ScreenPos，顶点阶段：

```text
o.pos = TransformObjectToHClip(v.vertex.xyz);
o.screen_uv = ComputeScreenPos(o.pos);
```

片元阶段：

```text
i.screen_uv.xy = i.screen_uv.xy / i.screen_uv.w;
float3 posWS_Re1 = DepthToWorldPositionV1(i.screen_uv.xy);   //NDC逆推重建世界坐标
```

DepthToWorldPositionV1( )：

```text
float3 DepthToWorldPositionV1(float2 screenPos)
{
    //screenPos / screenPos.w就是【0,1】的归一化屏幕坐标  //_CameraDepthTexture是获取的深度图
    //Linear01Depth将采样的非线性深度图变成线性的
    float depth = Linear01Depth(SAMPLE_DEPTH_TEXTURE(_CameraDepthTexture, sampler_CameraDepthTexture, screenPos), _ZBufferParams);
    //将【0，1】映射到【-1， 1】上，得到ndcPos的x，y坐标
    float2 ndcPosXY = screenPos * 2 - 1;
    //float3的z值补了一个1，代表远平面的NDC坐标  _ProjectionParams代表view空间的远平面, 我们知道裁剪空间的w和view空间的z相等，
    //相当于做了一次逆向透视除法，得到了远平面的clipPos
    float3 clipPos = float3(ndcPosXY.x, ndcPosXY.y, 1) * _ProjectionParams.z;

    float3 viewPos = mul(unity_CameraInvProjection, clipPos.xyzz).xyz * depth;  //远平面的clipPos转回远平面的viewPos， 再利用depth获取该点在viewPos里真正的位置
    //补一个1变成其次坐标，然后逆的V矩阵变回worldPos
    float3 worldPos = mul(UNITY_MATRIX_I_V, float4(viewPos, 1)).xyz;
    return worldPos;
}
```

### 2.相似三角形法

同NDC逆推法一样，先获取screenPos，同时还要在顶点阶段，多获取一个ViewSpace的posVS

```text
o.posVS = TransformWorldToView(TransformObjectToWorld(v.vertex.xyz));
```

DepthToWorldPositionV2( )：

```text
float3 DepthToWorldPositionV2(float2 screenPos, float3 posVS)
{
    //相对摄像机的深度：
    float depth = LinearEyeDepth(SAMPLE_DEPTH_TEXTURE(_CameraDepthTexture, sampler_CameraDepthTexture, screenPos), _ZBufferParams);

    //由于我们在视图空间下，视线方向始终垂直于XY平面，所以由相似三角形，很容易得出 i.posVS.z / waterObject.z = i.posVS.x / waterObject.x
    //其中waterObject.z即为depth
    //y同理，所以可以推导出i.posVS.z / waterObject.z = i.posVS.xy / waterObject.xy
    float2 waterObjectVS = depth / (-posVS.z) * posVS.xy;       //注意：i.posVS.z是负值，要加个负号变成正数
    float3 waterObjectWS = mul(UNITY_MATRIX_I_V, float4(waterObjectVS, -depth, 1)).xyz;     //注意Depth应该是负值，同时补齐齐次坐标的1（代表坐标）
    return waterObjectWS;
}
```

### 3.角向量插值法

①利用C#脚本获取四个角向量

```text
//  Description:通过深度图重建世界坐标，视口射线插值方式
using System.Collections;
using System.Collections.Generic;
using UnityEngine;

[ExecuteInEditMode]
public class ReconstructPositionViewPortRay : MonoBehaviour
{

    private Material postEffectMat = null;
    private Camera currentCamera = null;

    void Awake()
    {
        currentCamera = GetComponent<Camera>();
    }

    void OnEnable()
    {
        if (postEffectMat == null)
            postEffectMat = new Material(Shader.Find("lcl/Depth/ReconstructPositionViewPortRay"));
        currentCamera.depthTextureMode |= DepthTextureMode.Depth;
    }

    void OnDisable()
    {
        currentCamera.depthTextureMode &= ~DepthTextureMode.Depth;
    }

    void OnRenderImage(RenderTexture source, RenderTexture destination)
    {
        if (postEffectMat == null)
        {
            Graphics.Blit(source, destination);
        }
        else
        {
            var aspect = currentCamera.aspect;
            var far = currentCamera.farClipPlane;
            var right = transform.right;
            var up = transform.up;
            var forward = transform.forward;
            var halfFovTan = Mathf.Tan(currentCamera.fieldOfView * 0.5f * Mathf.Deg2Rad);

            //计算相机在远裁剪面处的xyz三方向向量
            var rightVec = right * far * halfFovTan * aspect;
            var upVec = up * far * halfFovTan;
            var forwardVec = forward * far;

            //构建四个角的方向向量
            var topLeft = (forwardVec - rightVec + upVec);
            var topRight = (forwardVec + rightVec + upVec);
            var bottomLeft = (forwardVec - rightVec - upVec);
            var bottomRight = (forwardVec + rightVec - upVec);

            var viewPortRay = Matrix4x4.identity;
            viewPortRay.SetRow(0, topLeft);
            viewPortRay.SetRow(1, topRight);
            viewPortRay.SetRow(2, bottomLeft);
            viewPortRay.SetRow(3, bottomRight);

            postEffectMat.SetMatrix("_ViewPortRay", viewPortRay);
            Graphics.Blit(source, destination, postEffectMat);
        }
    }
}
```

②顶点阶段确定是哪一个角落的向量，片元阶段会帮我们自动插值，所以直接拿来计算就好

```text
// ================================ 深度图重建世界坐标 ================================
//视口射线插值方式
// 推荐使用该方法，效率比较高
Shader "ReconstructPositionViewPortRay"
{
    CGINCLUDE
    #include "UnityCG.cginc"
    sampler2D _CameraDepthTexture;
    float4x4 _ViewPortRay;
    
    struct v2f
    {
        float4 pos : SV_POSITION;
        float2 uv : TEXCOORD0;
        float4 rayDir : TEXCOORD1;
    };
    
    v2f vertex_depth(appdata_base v)
    {
        v2f o;
        o.pos = UnityObjectToClipPos(v.vertex);
        o.uv = v.texcoord.xy;
        
        //用texcoord区分四个角，就四个点，if无所谓吧
        int index = 0;
        if (v.texcoord.x < 0.5 && v.texcoord.y > 0.5)
            index = 0;
        else if (v.texcoord.x > 0.5 && v.texcoord.y > 0.5)
            index = 1;
        else if (v.texcoord.x < 0.5 && v.texcoord.y < 0.5)
            index = 2;
        else
            index = 3;
        
        o.rayDir = _ViewPortRay[index];
        return o;
    }
    
    fixed4 frag_depth(v2f i) : SV_Target
    {
        // return fixed4(i.uv,0, 1.0);
        
        float depthTextureValue = SAMPLE_DEPTH_TEXTURE(_CameraDepthTexture, i.uv);
        float linear01Depth = Linear01Depth(depthTextureValue);
        //worldpos = campos + 射线方向 * depth
        float3 worldPos = _WorldSpaceCameraPos + linear01Depth * i.rayDir.xyz;
        return fixed4(worldPos, 1.0);
    }
    ENDCG
    
    SubShader
    {
        Pass
        {
            ZTest Off
            Cull Off
            ZWrite Off
            Fog
            {
                Mode Off
            }
            
            CGPROGRAM
            #pragma vertex vertex_depth
            #pragma fragment frag_depth
            ENDCG
        }
    }
}
```

## 参考

1. [^](https://zhuanlan.zhihu.com/p/92315967#ref_1_0) w is 1/FarPlane. https://docs.unity3d.com/Manual/SL-UnityShaderVariables.html
2. [^](https://zhuanlan.zhihu.com/p/92315967#ref_2_0)Unity's convention, where forward is the positive Z axis. https://docs.unity3d.com/ScriptReference/Camera-worldToCameraMatrix.html
3. [^](https://zhuanlan.zhihu.com/p/92315967#ref_3_0)https://www.jianshu.com/p/df878a386bec
4. [^](https://zhuanlan.zhihu.com/p/92315967#ref_4_0)https://forum.unity.com/threads/getting-scene-depth-z-buffer-of-the-orthographic-camera.601825/#post-4966334